---
layout     : post
title      : "MDNet"
subtitle   : "Learning Multi-Domain Convolutional Neural Networks for Visual Tracking"
date       : 2017-04-17
author     : "Tim Ho"
tags       : Deep-Learning Object-Tracking CVPR-2016
comments   : true
signature  : true
---

## 前言
Visual Tracking領域，CNN-based方法並沒有取得如Classification, Detection, Segmentation同樣的成功。
多數paper把原因指向目前仍沒有夠多的Training data，於是通常會以Transfer learning的方法，例如使用ImageNet的Pretrained model，只finetune後面幾層，學習dataset中各種物體的feature，再用於Visual tracking。

---

## 問題
然而這樣做的效果並不好，因為classification和tracking這兩個問題有著根本上的差異，在classification中，每張image中出現的物體我們都希望知道他是什麼，但在tracking中，我們只關心需要被追蹤的物體，假設畫面中同時存在物體`A`和`B`，當我們想追蹤`A`的時候，就希望把`B`視作背景，當我們想追蹤`B`了，又希望把`A`給視作背景，而且`A`和`B`還有可能是同一種物體，例如都是人或者車，所以拿classification的feature來transfer到tracking的問題，是存在某種程度mismatch的。

---

## 論文主要想法
這篇work提出MDNet，目的是希望學到domain independent feature，意在分辨被追蹤的物體及背景 (binary classification)，而物體是什麼其實不太重要。
主要方法是MD，MD是Multi-Domain的意思，將每部training video都當作一個domain，有自己的fully connect layer，自已獨立計算loss，而所有的這些分支會共享convolution layer，不管哪個分支的loss，backprop回來都會更新convolution layer的weight，藉此學到domain independent feature。

---

![]({{baseurl}}/images/mdnet/mdnet_structure.png)

## Network架構
Input固定resize成 `107 x 107 x 3`
三層conv layer，此三層使用VGG-M model weight做initial
兩層fc layer
最後分接domain specific fc layer，如有k部training video就會有k個分支

為什麼使用這麼小的network呢? 因為我們的問題只是binary classification，只要feature夠好應該不需要太複雜的function。
而要追蹤的物體通常占畫面比例較小，因此使用了 `107 x 107` 的input size。

---

## Training
每個iteration只會用其中一個domain的data，例如`1st iteration`用`video 1`的data ，所以最後的fc都走`k=1`這個brach，由這個brach的loss更新整個network，`2nd iteration`就用`video 2`的data，走`k=2`的brach，更新整個network，以此類推...

---

<div align="center">
<img src="{{baseurl}}/images/mdnet/algo.png" height="500" width="400" />
</div>

## Testing
我們的目的是想得到domain independent的feature，所以training完之後，會移除最後面所有的domain specific fc layer
真的要拿來追一個沒看過的物體的時候，會接上一個新的`fc layer`，進行online learning，讓network學會分這個物體和背景

* 第一個frame，給定要追蹤的物體的bounding box，在此bounding box周圍sample數個其他的bounding box，用來train bounding box regressor，這個regressor就會學到在小範圍內refine我們之後預測出的bounding box

* 之後每一個新的frame，都會由`前一個frame物體位置的周圍`，sample許多bounding box當作新的物體位置的`candidate`，每個bounding box都用network去預測是`物體`還是`背景`，最後取分數最大的。
MDNet的network雖然很小，但是tracking的速度其實很**慢**，一個原因是因為每一個frame都要跑這麼多可能的candidate。

而另一個慢的原因是需要online learning，
每個frame會預測很多個candidate，用兩個`queue`分別收集每個frame預測的positive和negative sample。

* `positive queue (long term queue)` 會收集比較長的時間，paper中是100個frame，因為既使時間長了環境改變，要追蹤的物體也不會變
* `negative queue (short term queue)` 只會收集最近20個frame，因為背景在追蹤過程中是一直改變的

當loss track或者固定一段間隔，就會用這些data來做network的更新。
從queue中選data的時候會搭配`hard negative mining`，這個技巧的想法是，如果都餵很容易分辨的背景給network，對於feature的學習不太有幫助，所以會從`false positive`，也就是挑把背景誤判成物體的sample，讓network的辨別力更好。

<div align="center">
<img src="{{baseurl}}/images/mdnet/hnm.png"/>
</div>

根據paper上面的具體數據，在Xeon E5-2660和K20m之下大約是1 FPS(Matlab code)，不過準確度還是很不錯的，如果事先知道要追什麼物體，免除online learning的需要，就有更多能用的場景了。

<div align="center">
<img src="{{baseurl}}/images/mdnet/result.png"/>
</div>









